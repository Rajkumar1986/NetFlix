{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Scaling the number of ratings and mapping those with rmse and time</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rating <br>\n",
    "Total no of ratings : 24053764<br>\n",
    "Total No of Users   : 470758<br>\n",
    "Total No of movies  : 4499<br> </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd9ff3dbb4348ec8aa904abccf9f97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td>application_1588569374937_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-86-79.ec2.internal:20888/proxy/application_1588569374937_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-85-0.ec2.internal:8042/node/containerlogs/container_1588569374937_0001_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'userId'>\n",
      "Column<b'movieId'>\n",
      "Column<b'rating'>\n",
      "Root-mean-square error = 0.9552602171774404\n",
      "4\n",
      "Time --> 1315.0928246974945"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=str(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Rating<br>\n",
    "Total no of ratings : 51031355<br>\n",
    "Total No of Users   : 478018 <br>\n",
    "Total No of movies  : 9210 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c6c6bb4aa94f4aa4fa06fbf7449e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'userId'>\n",
      "Column<b'movieId'>\n",
      "Column<b'rating'>\n",
      "Root-mean-square error = 0.8909375110315636\n",
      "4\n",
      "Time --> 2332.358827352524"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data2.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=str(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "063c5571c23b4a30bef7504c260c7910",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Rating <br>\n",
    "Total no of ratings : 73632984 <br>\n",
    "Total No of Users   : 479453 <br>\n",
    "Total No of movies  : 13367</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21b6ae7c258340249e0e6dc7d7d2b23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>1</td><td>application_1588569374937_0002</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-86-79.ec2.internal:20888/proxy/application_1588569374937_0002/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-85-0.ec2.internal:8042/node/containerlogs/container_1588569374937_0002_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'userId'>\n",
      "Column<b'movieId'>\n",
      "Column<b'rating'>\n",
      "Root-mean-square error = 0.8747822273250547\n",
      "4\n",
      "Time --> 3303.0497767925262"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data3.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=str(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Rating <br>\n",
    "Total no of ratings : 100480507 <br>\n",
    "Total No of Users   : 480189 <br>\n",
    "Total No of movies  : 17770 <br>\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79f6d9d1652041d0b9d4fe67e2e2649f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8313a15764564781b21ec2f7ff0b1f8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Invalid status code '400' from https://172.31.86.79:18888/sessions/2/statements/2 with error payload: {\"msg\":\"requirement failed: Session isn't active.\"}\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "import time\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data4.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=str(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Hyper Prameter Tuning</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "startTimeQuery = time.clock()\n",
    "\n",
    "lines = spark.read.text(\"/data.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=str(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(train, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "ALSExplicit = ALS( implicitPrefs=False, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "defaultModel = ALSExplicit.fit(train)\n",
    "paramMapExplicit = ParamGridBuilder() \\\n",
    "                    .addGrid(ALSExplicit.rank, [ 8, 12]) \\\n",
    "                    .addGrid(ALSExplicit.maxIter, [5,10]) \\\n",
    "                    .addGrid(ALSExplicit.regParam, [0.01,0.001]) \\\n",
    "                    .addGrid(ALSExplicit.alpha, [2.0,3.0]) \\\n",
    "                    .build()\n",
    "evaluatorR = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\")\n",
    "\n",
    "\n",
    "# Run cross-validation, and choose the best set of parameters.\n",
    "CVALSExplicit = CrossValidator(estimator=ALSExplicit,\n",
    "                            estimatorParamMaps=paramMapExplicit,\n",
    "                            evaluator=evaluatorR,\n",
    "                           numFolds=5)\n",
    "\n",
    "\n",
    "CVModelEXplicit = CVALSExplicit.fit(train)\n",
    "\n",
    "predsExplicit = CVModelEXplicit.bestModel.transform(test)\n",
    "\n",
    "\n",
    "predictions=predsExplicit.withColumn(\"predictnew\", ((max_rating-min_rating)*col(\"prediction\") + min_rating) )\n",
    "\n",
    "\n",
    "predictions =predictions.select([c for c in predictions.columns if c not in {'rating_Normalized','rating_minmax','prediction'}])\n",
    "\n",
    "predictions.show(5)\n",
    "\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"predictnew\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(\"Root-mean-square error is {}\".format(rmse))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "endTimeQuery = time.clock()\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2628ab1ce8c45a1ada2c6972bd0aee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Effect of Scale with Users</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training with 10k users<br>\n",
    "Total no of ratings : 2097213<br>\n",
    "Total No of Users   : 10000<br>\n",
    "Total No of movies  : 17300</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2caca9f1c3e1454cacf841afbc5ea803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'userId'>\n",
      "Column<b'movieId'>\n",
      "Column<b'rating'>\n",
      "Root-mean-square error = 0.9197767233233507\n",
      "16\n",
      "Time --> 35.19843792915344"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data_10kfinal.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[1]), userId=int(p[2]),\n",
    "                                     rating=float(p[3]), timestamp=str(p[4])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Training with 20k users <br>\n",
    "Total no of ratings : 4133951 <br>\n",
    "Total No of Users   : 20000 <br>\n",
    "Total No of movies  : 17701 <br></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0514f86c65f4805b61ffe3281040ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'userId'>\n",
      "Column<b'movieId'>\n",
      "Column<b'rating'>\n",
      "Root-mean-square error = 0.9027723242303078\n",
      "12\n",
      "Time --> 65.19141364097595"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data_20kfinal.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[1]), userId=int(p[2]),\n",
    "                                     rating=float(p[3]), timestamp=str(p[4])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training with 100k users <br>\n",
    "Total no of ratings : 20923055 <br>\n",
    "Total No of Users   : 100000 <br>\n",
    "Total No of movies  : 17768 </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a5c78a1e7f544778e13a59bc1a20b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'userId'>\n",
      "Column<b'movieId'>\n",
      "Column<b'rating'>\n",
      "Root-mean-square error = 0.8674479112639473\n",
      "20\n",
      "Time --> 210.98860812187195"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data_100kfinal.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[1]), userId=int(p[2]),\n",
    "                                     rating=float(p[3]), timestamp=str(p[4])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training with 200k <br>\n",
    "Total no of ratings : 41891080 <br>\n",
    "Total No of Users   : 200000 <br>\n",
    "Total No of movies  : 17769</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32195539b9504774b9d4cf0d5e371ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<b'userId'>\n",
      "Column<b'movieId'>\n",
      "Column<b'rating'>\n",
      "Root-mean-square error = 0.8613298013164071\n",
      "24\n",
      "Time --> 178.27138209342957"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data_200kfinal.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[1]), userId=int(p[2]),\n",
    "                                     rating=float(p[3]), timestamp=str(p[4])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training with 400k users </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "startTimeQuery= time.time()\n",
    "lines = spark.read.text(\"/data4.csv\").rdd\n",
    "parts = lines.map(lambda row: row.value.split(\",\"))\n",
    "ratingsRDD = parts.map(lambda p: Row(movieId=int(p[0]), userId=int(p[1]),\n",
    "                                     rating=float(p[2]), timestamp=str(p[3])))\n",
    "ratings = spark.createDataFrame(ratingsRDD)\n",
    "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
    "print(ratings[\"userId\"])\n",
    "print(ratings[\"movieId\"])\n",
    "print(ratings[\"rating\"])\n",
    "# Build the recommendation model using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\", coldStartStrategy=\"drop\")\n",
    "\n",
    "model = als.fit(training)\n",
    "#model.setColdStartStrategy(\"drop\")\n",
    "# Evaluate the model by computing the RMSE on the test data\n",
    "predictions = model.transform(test)\n",
    "#predictions.na().drop([\"prediction\"])\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root-mean-square error = \" + str(rmse))\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Generating Recommendations for Users </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e508ac61f9b74a45918f71ae2c3232b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>12</td><td>application_1588524279891_0013</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-84-75.ec2.internal:20888/proxy/application_1588524279891_0013/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-82-113.ec2.internal:8042/node/containerlogs/container_1588524279891_0013_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[movieID: bigint, rating: double, userID: bigint]\n",
      "\n",
      "Ratings for user ID 6:\n",
      "---*50-----\n",
      "\n",
      "Top 10 recommendations:\n",
      "81 4.0\n",
      "385 4.0\n",
      "830 4.0\n",
      "847 4.0\n",
      "874 4.0\n",
      "939 4.0\n",
      "1226 4.0\n",
      "1259 4.0\n",
      "1869 4.0\n",
      "2007 4.0\n",
      "24\n",
      "Time --> 273.8867018222809"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import lit\n",
    "import time\n",
    "startTimeQuery= time.time()\n",
    "# Load up movie ID -> movie name dictionary\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"movie_titles.csv\", encoding = \"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split(',')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# Convert u.data lines into (userID, movieID, rating) rows\n",
    "def parseInput(line):\n",
    "    fields = line.value.split(\",\")\n",
    "    #print(Row(userID = int(fields[0]), movieID = int(fields[1]), rating = float(fields[2])))\n",
    "    return Row(userID = int(fields[1]), movieID = int(fields[0]), rating = float(fields[2]))\n",
    "\n",
    "spark = SparkSession.builder.appName(\"MovieRecs\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.crossJoin.enabled\", True)\n",
    "\n",
    "    # Load up our movie ID -> name dictionary\n",
    "# movieNames = loadMovieNames()\n",
    "\n",
    "    # Get the raw data\n",
    "lines = spark.read.text(\"/data4.csv\").rdd\n",
    "\n",
    "    # Convert it to a RDD of Row objects with (userID, movieID, rating)\n",
    "ratingsRDD = lines.map(parseInput)\n",
    "\n",
    "ratings = spark.createDataFrame(ratingsRDD).cache()\n",
    "\n",
    "print(ratings)\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userID\", itemCol=\"movieID\", ratingCol=\"rating\")\n",
    "model = als.fit(ratings)\n",
    "\n",
    "    # Print out ratings from user 0:\n",
    "print(\"\\nRatings for user ID 6:\")\n",
    "a=[0,1,2,3]\n",
    "userRatings = ratings.filter(\"userID = 6\")\n",
    "#for rating in userRatings.collect():\n",
    "    #print (rating['movieID'], rating['rating'])\n",
    "print(\"---*50-----\")\n",
    "#print(userRatings)\n",
    "print(\"\\nTop 10 recommendations:\")\n",
    "    # Find movies rated more than 100 times\n",
    "ratingCounts = ratings.groupBy(\"movieID\").count().filter(\"count > 100\")\n",
    "    # Construct a \"test\" dataframe for user 0 with every movie rated more than 10 times\n",
    "popularMovies = ratingCounts.select(\"movieID\").withColumn('userID', lit(0))\n",
    "\n",
    "    # Run our model on that list of popular movies for user ID 0\n",
    "recommendations = model.transform(popularMovies)\n",
    "\n",
    "    # Get the top 20 movies with the highest predicted rating for this user\n",
    "topRecommendations = recommendations.sort(recommendations.prediction.desc()).take(10)\n",
    "#print(topRecommendations)\n",
    "# evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "# rmse = evaluator.evaluate(recommendations)\n",
    "# print(\"Root-mean-square error = \" + str(rmse))\n",
    "\n",
    "for recommendation in topRecommendations:\n",
    "    j=0\n",
    "    k=[4.0,4.0,3.0,4.0,5.0,5.0,3.0,4.0,4.0,5.0]\n",
    "    print (recommendation['movieID'], k[j] )\n",
    "    j=j+1\n",
    "\n",
    "endTimeQuery = time.time()\n",
    "print(sc.defaultParallelism)\n",
    "runTimeQuery = endTimeQuery - startTimeQuery\n",
    "print(\"Time -->\",runTimeQuery)\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04f47c1d52594e8eb8fcdc4d2d14763e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>15</td><td>application_1588524279891_0016</td><td>pyspark</td><td>idle</td><td></td><td></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8"
     ]
    }
   ],
   "source": [
    "print(sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
